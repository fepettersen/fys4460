\documentclass[a4paper,english, 10pt, twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsfonts, amssymb, amsmath}
\usepackage{listings}
\usepackage{float}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}

%opening
\title{Exam preperations FYS4460}
\author{Fredrik E Pettersen\\ f.e.pettersen@fys.uio.no}
\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage

\section{Molecular-dynamics algorithms}
There are several different possibilities for ``variation'' when doing molecular dynamics. First of all 
one can use different potenitals according to what one would like to model. The simplest beeing the 
Lennard-Jones potential, which only includes two-particle interactions. The Lennard-Jones potential is 
typically pretty good for simulations of noble gasses. If we should want to study another material we will 
therefore need another potential. The weber-Stillinger potential includes both two- and three-particle 
interactions, and can model silicone (Si). Silicone will, in equilibrium, form 4-coordinated tetrahedral 
structures, which the Weber-Stillinger potential reproduces. The VKRE (Vashista, Kalia, Rino, and Ebbsjö) 
potiential is another potential which includes both two and three particle interactions, but this potential 
is made (specificly) to simulate $SiO_2$. The two-body part includes three terms, the Coulomb interaction, 
steric repulsion due to ionic sizes, and a charge-dipole interaction resulting from the large
electronic polarizability of $O_2^-$. Yet another example of a possible potential is the reaxFF potential 
which also includes four-particle interactions. This potential lets us study water molecules in an $SiO_2$ 
matrix.\\
In our Lennard-Jones studies we used the Verlet algorithm for time-integration. It is a simple, yet quite 
good integrator which conserves energy very well. It can be viewed as an advanced Euler-Chromer method, 
where we first update the velocity of each particle at half the timestep, then the position is integrated 
one full timestep using the new velocity. When this is completed for all paritcles we calculate two-body 
forces using the new positions before we use the new forces to fully integrate the velocities.\\
All of the potentials mentiones above decrease in strength with the distance between atoms. For the Lennard-
Jones potential, this is illustrated in figure \ref{LJ_cut}. As we see from this figure, there is for all 
practical purposes no interaction between particles that are a certain length apart. This means that we 
can skip the calculations of forces (which is very expensive) between theese particles. As a practical 
measure we will therefore divide the system we are simulating into cells which are the same size as the 
cutoff length and only calculate forces between particles in neighbouring cells. In this way we are sure 
that no important calculations are left out, and still gain a significant speedup. In fact, should we do 
all pairwise force calculations we will at best need something like $\frac{n^2}{2}$ FLOPS, where as we 
will only need something like $n$ FLOPS when we use cells.\\
We initialize the system by placing it in a face centered cubic lattice (figure \ref{fcc}) and give each 
atom a random velocity drawn from the uniform distribution. We will of course also need to remove any 
initial drift in the system also by rescaling all velocities by the mean of all velocities. Finally, to 
get around the boundary problems, where any potential we have chosen will strictly speaking be incorrect, 
we implement periodic boundary conditions. That is, two particles on each side of the lattice are neighbouring.

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{plot_potential.png}
\caption{Strength of Lennard-Jones potential as a function of distance between atoms}
\label{LJ_cut}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.45]{fcc_lattice.png}
\caption{Face centered cubic lattice}
\label{fcc}
\end{figure}

\section{Molecular-dynamics in the micro-canonical ensemble}\label{section_2}
If we leave the simulation to itself after setting it up, the temperature will drop quite dramatically 
and then fluctuate around some steady state solution. This is first off all unwanted, and (possibly) 
unphysical. One way to solve this problem is by introducing a thermostat. In theese simulations we are 
in the microcanonical ensemble. That is, The number of particles, volume, and energy are all fixed. In 
the microcanonical ensemble it can be shown that the temperature is 
\begin{equation}\label{temperature}
T = \frac{2E_k}{3N}
\end{equation}

where the kinetic energy is the sum of the kinetic energy off all the particles. This means that we can 
manipulate the temperature of the system by rescaling the velocity of each particle. There are, as usual, 
a few alternatives on how to do this. The berendsen thermostat lets us simulate the canonical ensemble (poorly) by 
``surrounding'' our system with a heat bath, and rescaling the velocities accordingly. The scaling constant 
is shown in eq \ref{berendsen}.
\begin{equation}\label{berendsen}
 \gamma = \sqrt{1+\frac{\Delta t}{\tau}\left(\frac{T_{bath}}{T}-1\right)}
\end{equation}
Another possibility is the Andersen thermostat which simulates collitions between atoms inside the system and in 
the heat bath. Atoms which collide will gain a new, normally distributed velocity with standard deviation 
$\sqrt{k_B T_{bath}/m}$. For all atoms a random, uniformly distributed number is drawn. If the number is less than 
$\frac{\Delta t}{\tau}$, the atom is assigned a new velocity. A downside of the Andersen thermostat is that 
it disturbs the dynamics of lattice vibrations.

\section{Molecular-dynamics in the micro-canonical ensemble}
The ultimate goal of molecular dynamics simuations is to measure macroscopic quantities such as the pressure, 
permeability, viscosity etc. from the microscopical simulations. This can be achieved via statistical mechanics. 
As mentioned in section \ref{section_2} the temperature can be expressed from the microcanonical ensemble as 
equation \ref{temperature} (Insert mathematical argument).\\
In a similar way we can find the pressure of the entire system as it is expressed in equation \ref{pressure}.
\begin{equation}\label{pressure}
p = \vec{F}\cdot\vec{r} 
\end{equation}
Theese quantities can for example be used to say something about how a system will react to a fluid beeing 
driven through it.

\section{Measuring the diffusion constant in molecular-dynamics simulations}\label{section_4}
We can measure the diffusion constant by measuring the mean square displacement of the particles in the system. 
This will relate to the diffusion constant as 
\begin{equation}\label{einstein}
 \langle r^2\rangle \simeq 2dtD
\end{equation}
where d denotes the dimensionality of the system (in most cases 3), t is directly relatable to the number of 
timesteps that have passed and D is the diffusion constant. One problem by using this relation is that, to 
my knowledge, it has only been derived for particles in a homogenous fluid which is much larger than the average 
size of the particles. Also, one must account for the periodic boundary conditions. This is done by summing the 
distance traveled by each particle at each timestep, and carefully accounting for ``jumping'' from one side of 
the system to the other.\\
Equation \ref{einstein} is also widely used to measure the diffusion constant in random walks computations. 
For example we ca use it to measure the diffusion constant of random walkers on a spanning cluster in percolation 
theory. INSERT SOME MORE STUFF

\section{Measuring the radial distribution function in molecular-dynamics simulations}
The radial distribution function can be measured in by counting the number of particles in spherical shell with 
increasing radius. \\
To calculate the pair distribution function from a simulation, the neighbors around each atom or molecule are sorted 
into distance bins. The number of neighbors in each bin is averaged over the entire simulation. For example, a count 
is made of the number of neighbors between $2.5$ and $2.75$, $2.75$ and $3.0$ Å and so on for every atom or molecule 
in the simulation. This count can be performed during the simulation itself or by analyzing the configurations that 
are generated. \\
The radial distribution function tells us something about the density distribution in a material, and can be used to 
calculate several macroscopic quantities such as the pressure, structure factor, compressibility, etc.
Diffuculties in measuring $g(r)$ include it beeing computationally intensive, troubles with getting the scaling right 
(I still have not gotten it right), and similar problems.
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{radial_distr.png}
    \caption{Liquid at $T=100$K}
    \label{g(r):liquid} 
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{radial_distr2.png}
    \caption{Solid}
    \label{g(r):solid} 
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{radial_distr3.png}
    \caption{Dilute gas as $T=300$K}
    \label{g(r):gas} 
\end{subfigure}
\caption{Radial distriburion function for liquid, solid and gas. x-axis shows distance in $\sigma$}
\label{g(r)}
\end{figure}

\section{Thermostats in molecular-dynamics simulations}
As mentioned in section \ref{section_2} the system will be represented by the microcanonical ensemble if we leave it to 
itself. We can adjust the system so that it is represented by the canonical ensemble by ``surrounding'' it with 
a heat bath. This is done by introducing a thermostat which (for example) rescales the velocities of the particles 
so that the temperature of the system matches that of the heat bath. The best thermostat for simulations of the 
canonical ensemble is the Nosè-Hoover thermostat, however this is (apparently) rather difficult to implement.\\
In section \ref{section_2} two other alternatives are described, the Andersen and the Berendsen thermostats.

\section{Generating a nano-porous material}
Generally we generate a nanoporous matrix by first thermalizing a ``normal'' system at some temperature. 
Next, we need some algorithm to make it porous. A typical way of doing this is by removing some of the particles 
in the system by some rule, or at least mark them as stationary. The pores can be specified by, for example, 
drawing spheres of a random radius placed at a random position. Atoms outside the sphere are marked as stationary 
(or the other way around). In more complex simulations one usually heats the system a lot, expands it and rapidly cools 
it again. This will give a more realistic geometry.\\
The structure of a nanoporous matrix can be characterised by its porosity (which we have a somewhat poor measure for), but 
this raises the problem of having several possible permeabilities for each porosity. One could have all the pores in one 
large sphere (no permeability), or in one large cylindrical pore which has a large permeability. The fluid is typically 
characterized by its viscosity and its density. This also poses somewhat of a problem in that the viscosity is not 
independent of the geometry of the matrix or (by extension) its permeability. It is also ``strongly'' dependent on 
density. In short the viscosity is a hard measurment.

\section{Diffusion in a nano-porous material}
The diffusion constant is measured in the same way as we described in section \ref{section_4}. \\
I guess one could expect the diffusion constant to depend on the porosity of the system. 
It would also make sense if the diffusion constant was different in different directions because the connectivity might depend on direction. This 
should (at least for randomly generated matrices) cancel out on larger scales. \\
Diffusion in larger scale porous media is governed by nonlinear diffusion equations which are immensely complicated 
to model (using upwind differences etc). However, the thing all diffusion processes have in common is reaching a steady 
state after some time. This is also expected from the low density fluid in the nanoporous system.

\section{Flow in a nano-porous material}
We can introduce flow in one direction in the nanoporous material by adding a driving force to all particles, much like a gravitational force.
This should make all particles drift in one direction.\\
The model can be checked by simulating the flow through a cylindrical pore. From continuum mechanichs we know that the velocity profile for 
such a system should be a parabola, and we expect this to be the case for our MD simulations as well. Figure \ref{vel_profile} shows this 
if you really put your imagination into it.\\
To calculate the viscosity of the model we need the superficial velocity distribution of the fluid. One way to estimate the fluid viscosity is 
by simulating flow through a cylindrical pore of a given radius and with a given driving force we find the viscosity from
\begin{align*}
 u(r) &= \frac{\Delta P}{4\mu L}\left(a^2-r^2\right)\\
 \implies \mu &= \frac{nFa^2}{4u(0)}
\end{align*}
The permeability of the nanoporous material can be measured by counting the flux of particles through one part of the system (say one square 
area). The easiest part to pick is the periodic boundary. We have from Darcy's law that
\begin{equation}
 \frac{Q}{V} = \frac{k}{\mu}\left(\frac{\Delta P}{L}-\rho g\right) = \frac{kN}{\mu V}F \Longleftrightarrow k = \frac{\mu Q}{NF}
\end{equation}

\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{project2_velocitydistr_0800.png}
\caption{$v_z$ for all moving particles projected onto a radius centered in the middle of the cylinder.}
\label{vel_profile}
\end{figure}


\section{Algorithms for percolation systems}
To generate percolation systems for simulation we need a (square) matrix of random numbers. Commonly theese are drawn from the 
uniform distribution. Next we will need to set a porosity for the system, and mark all matrix entries smaller than the porosity 
as filled sites, and all entries larger than the porosity as unfilled sites. This gives us clusters of filles sites which have 
neighbours that are filled as well (or clusters of connected, filled sites), see figure \ref{perc_matrix}. Theese clusters are labelled with a number.
We then find the spanning cluster (should it exist) by checking if the same label exists at both sides of the matrix (and remove the label 0). 
The percolation probability, which is the ptobability of there beeing a spanning/percolating cluster, is measured by checking wther or not there is 
a percolating cluster, and averaging the measurments over several matrices.

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{oppg_a_percmatrix.png}
\caption{Percolation matrix of size $800\times800$ with porosity $p=0.6$}
\label{perc_matrix}
\end{figure}

\section{Percolation on small lattices}
If we want to study percolation on vey small lattices, say $2\times2$ lattices, we can find excact solutions for $\Pi(p,L)$. As we found 
in project 3, $\Pi(p,L)$ goes to a step function as L goes to infinity. Similarly, we would expect $\Pi(p,L)$ to go to a polynomial of 
degree 1 as L goes to 1. This can be shown mathematically.\\
The porosity of a $1\times1$ s system is either 0 or 1, in which case the percolation probability is either 0 or 1. This means that the 
percolation probability has to be a straight line from $(0,0)$ to $(1,1)$ since we cannot well define any ``path'' the function should take.
For the $2\times2$ case the system can be in either of the states shown in figure \ref{L2}.

\begin{figure}[H]
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{c1.png}
    \caption{c=1}
    \label{L2:c1}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{c5.png}
    \caption{c=2, degeneracy 4}
    \label{L2:c2}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{c3.png}
    \caption{c=3, degeneracy 2}
    \label{L2:c3}
  \end{subfigure}
  
  \begin{subfigure}[b]{0.3\textwidth}
   \includegraphics[width=\textwidth]{c4.png}
   \caption{c=4, degeneracy 4}
   \label{L2:c4}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
   \includegraphics[width=\textwidth]{c2.png}
   \caption{c=5, degeneracy 4}
   \label{L2:c5}
  \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{c6.png}
    \caption{c=6}
    \label{L2:c6}
  \end{subfigure}
  \caption{Possible states and their degeneracy for a $2\times2$ system}
  \label{L2}
\end{figure}
From figure \ref{L2} we can find $\Pi$ directly
\begin{align*}
 \Pi &= 0\cdot1\cdot p^0(1-p)^4 + 0\cdot4\cdot p^1(1-p)^3 + 0\cdot2\cdot p^2(1-p)^2 +\\
 1\cdot4&\cdot p^2(1-p)^2 + 1\cdot4\cdot p^3(1-p)^1 + 1\cdot1\cdot p^4(1-p)^0 \\
 \Pi &=4\cdot p^2(1-p)^2 + 4\cdot p^3(1-p) + p^4
\end{align*}

\section{Cluster number density in 1-d percolation}\label{section_12}
The cluster number density $n(s,p)$ is defined as the probability for a site to be a specific site (say the leftmost) in a cluster 
of size s. The probabiity of this is quite simply defined from equation \ref{nsp_def}; there are s sites filled, with a probability 
p, and one unfilled site on each site, with a probability (1-p). The probability for this site to be any part of a cluster of size s 
is $s\cdot n(s,p)$.
\begin{equation}\label{nsp_def}
 n(s,p) = (1-p)p^s(1-p) = p^s(1-p)^2
\end{equation}
If we write the number of clusters of size s as $N_s$ and the number of filled sites $\mathcal{N} = pL$, this should logically be 
equal to the sum over all clustersizes times the number of clusters of that size $pL = \sum\limits_{s=1}^\infty sN_s$. We can now 
esitmate the probability for a given, filled site to belong to a cluster of size s as 
\begin{equation*}
 (sn(s,p))' = \frac{sN_s}{\mathcal{N}}
\end{equation*}
Now, in order to estimate whether a given site (filled or not) to be a part of a cluster of size s, all we need to do is multiply 
by p, giving us $sn(s,p) = \frac{psN_s}{pL} \implies n(s,p) = \frac{N_s}{L^d}$, where d is the dimensionality of our system.\\
Rewwriting eqution \ref{nsp_def} gives us $n(s,p) = (1-p)^2e^{s\ln(p)} = (1-p)^2e^{-s/s_\xi}$ where $s_\xi = \frac{-1}{\ln(p)} 
\simeq (p_c -p)^{-1/\sigma}$. As we can see, $s_\xi$ is going to diverge as $p\to p_c$. This will make $n(s,p)$ a decaying exponential 
function with decreasing slope.\\
The expression $n(s,p) = \frac{N_s}{L^d}$ is the same as we use to measure the cluster number density in 2-D. The resulting curves 
are also similar (see f.eks figure \ref{nsp}).
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{cluster_numberdensity_small_p}
\caption{Cluster number density measured for $p<p_c$.}
\label{nsp}
\end{figure}

\section{Correlation length in 1-d percolation}
The correlation function $g(r)$ describes the probability for two sites separated by a distace r to belong to the same cluster, given 
that both sites are filled. I 1-D, the only way for this to happen is if all the sites between the two sites are filled as well. This 
means that we get 
\begin{equation}
g(r) = p^r = e^{r\ln(p)} = e^{\frac{-r}{\xi}}
\end{equation}
Where we, again, set $\xi = \frac{-1}{\ln(p)}$. $\xi$ diverges as $p\to 1=p_c$. Utilizing this, we can find the way it diverges by 
expanding $\ln(p) \to_{p\to p_c} -(p_c-p)$. This gives $\xi = \xi_0(p_c-p)^{-\nu}$ with $\nu = 1$

\section{Cluster size in 1-d percolation}
As mentioned in section \ref{section_12}, the characteristic clustersize, $s_\xi$, is defined from the cluster number density where 
it enters as $n(s,p) = (1-p)^2e^{-s/s_\xi}$ and $s_\xi = (p_c-p)^{-1/\sigma}$ where $\sigma=1$ in 1-d. From this it is obvious that $s_\xi$ 
will diverge as $p\to p_c$. In two dimensions, this is true when $p\to p_c$ both from above and below. \\
In the 2-d simulations we can (not without trouble) measure $\sigma$ either by plotting $n(s,p)$ as a function of s and measure where 
$n(s,p)$ is halved, or we can cheat by making a datacollapse plot (rescaling the axes by $s_\xi$) for various values of $\sigma$ untill 
we find a decent value. This will give us $s_\xi$ since we have all values entering in its expression. See figure \ref{sigma}.
\begin{figure}[H]
\centering
\begin{subfigure}[b]{\textwidth}
\includegraphics[scale=0.5]{sigma_10values.png}
\caption{Data collapse plot estimating $\sigma \simeq 0.4$.}
\end{subfigure}

\begin{subfigure}[b]{0.9\textwidth}
 \includegraphics[scale=0.5]{sigma_7values.png}
 \caption{Data collapse plot estimating $\sigma \simeq 0.4$ using a more correct algorithm.}
\end{subfigure}
\caption{Data collapse plots}
\label{sigma}
\end{figure}



\section{Measurment and behavior of $P(p,L)$ and $\Pi(p,L)$}\label{section_15}
The percolation probability is the simplest quantity to describe the behavior of of finite matrices. As we have 
allready mentioned, $\Pi(p,L)\to_{L\to\infty}H(p-p_c)$, and will be more like a hyperbolic tangent for small L. 
This is measured in the trivial way of averaging over several matrices (see figure \ref{sect_15:pi}). The probability 
for a site to belong to the percolating cluster $P(p,L)$ will of course be zero as long as there is no percolating 
cluster, and will then increase very repidly for a little while before (more or less) linearly going to 1 as the 
porosity of the system goes to 1. $P(p,L)$ will behave (naturally) in much the same way as $\Pi(p,L)$ does for finite 
system sizes. This quantity is also measured in the trivial way of counting and averaging.\\
We can make a similar finite size scaling theory for $P(p,L)$ from the relation $P(p,L) \propto \frac{M(p,L)}{L^d}$. 
We also have for $M(p,L)$:
\begin{equation*}
M(p,L) \propto \begin{cases}
	    \xi^{D-d}L^d & , L>>\xi \\
	    L^D & , L<<\xi
               \end{cases}
\end{equation*}
Inserting for $\xi \propto (p-p_c)^{-\nu}$ gives us 
\begin{equation*}
P(p,L) \propto \begin{cases}
	    (p-p_c)^{-\nu(D-d)}L^d & , L>>\xi \\
	    L^{D-d} & , L<<\xi
               \end{cases}
\end{equation*}

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.48\textwidth}
 \includegraphics[width=\textwidth]{oppg_i_Pi.png}
 \caption{$\Pi(p,L)$ for different system sizes.}
 \label{sect_15:pi}
\end{subfigure}
\begin{subfigure}[b]{0.48\textwidth}
  \includegraphics[width=\textwidth]{oppg_a_P.png}
 \caption{$P(p,L)$ for different system sizes.}
 \label{sect_15:P}
\end{subfigure}
\caption{blergh}
\label{sect_15}
\end{figure}

\section{The cluster number density}
As we discussed in section \ref{section_12} the cluster number density is the probability for a given site to be a specific 
site in a cluster of size s. In 1-d this is given by equation \ref{nsp_def}, but this will (of course) break down in more than 
1-d because itwill be more difficult to define the boundary, and because there are several ways of having a cluster of size s. 
To meet this challenge, we can in sted set $n(s,p) = \sum\limits_t g_{s,t}p^s(1-p)^t$, where $g_{s,t}$ is the degeneracy of the 
custer of size s, and t is all the possible numbers of neighbours.\\
The cluster number density is still measured in much the same way as in 1-d, with some more details. We will now have to do 
logarithmic binning of the cluster size, s, beacause most of the cluster sizes will be rather small, but a few will be very large. 
So we divide the ``s-axis'' into increasingly larger bins, and count the number of clusters within each bin.
$$
n(s,p)\Delta s = \frac{N_s}{L^d}
$$
Dividing by the bin size gives us our estimate for $n(s,p)$.\\
We can estimate $\sigma$ by producing a data collapse plot of $n(s,p)$ as shown in figure \ref{sigma}.

\begin{figure}[H]
 \centering
 \includegraphics[scale=0.8]{cluster_numberdensity_revisited.png}
 \caption{The cluster number density behaves as shown with increasing system size.}
\end{figure}

\section{Finite size scaling of $\Pi(p,L)$}
As we discussed in section \ref{section_15}, the percolation probability will behave in a predictable manner for finite 
system sizes. This is a characteristic we can make good use of to measure important exponents with high precicion, and also 
maesure the percolation threshold.
\begin{align*}
 \Pi(p,L) &= \xi^0f\left(\frac{L}{\xi}\right) = f\left(\frac{L}{\xi}\right)\\
 &= f\left(\frac{L}{\xi_0(p-p_c)^{-\nu}}\right)\\
 &= f\left(\frac{L(p-p_c)^\nu}{\xi_0}\right)\\
  &= \tilde{f}\left(\left[L^{1/\nu}(p-p_c)\right]^\nu\right)\\
  &= \Phi\left[\left(p-p_c\right)L^{1/\nu}\right]
\end{align*}
Where we have used that $\xi = \xi_0(p-p_c)^{-\nu}$ when $p>p_c$, $\tilde{f}(u) = f\left(\frac{u}{\xi_0}\right)$ and $\Phi(u)
= \tilde{f}(u^\nu)$. This scaling ansatz can be used as follows. We start by fixing $p$ which gives $\Pi(p) = x$. This can be 
denoted as 
\begin{equation}
 p_{\Pi=x}\left(L\right)
\end{equation}
The scaling ansatz now gives us
\begin{equation}
 x = \Phi\left[\left( p_{\Pi=x}\left(L\right)-p_c\right)L^{1/\nu}\right]
\end{equation}
which we solve as 
$$
\left(p_{\Pi=x}\left(L\right)-p_c\right)L^{1/\nu} = \Phi^{-1}(x) = C_x
$$
and finally rewrite as
\begin{equation}\label{measure_nu}
 p_{\Pi=x}\left(L\right)-p_c = C_xL^{-1/\nu}
\end{equation}
From equation \ref{measure_nu} we see that we can easily measure $\nu$ by doing a double logarithmic plot. This should give a 
straight line which has the incline $\frac{-1}{\nu}$. \\
If we now reverse the situation we can find $p_c$ by plotting $L^{-1/\nu}$ as a function of $p$. This should also give a straight 
line which intersects the y-axis at $p_c$. An example of this is shown in figure \ref{measure_pc}.
\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{oppg_k_rough.png}
\caption{Estimating $p_c$. This is only a rough estimate, where some 5-10 matrices have been used to average the measurments. 
This plot puts $p_c$ somwhere in the interval $[0.5923, 0.594]$}
\label{measure_pc}
\end{figure}

\section{Subsets of the spanning cluster}
To look at subsets of the spanning cluster we will need to look at flow on the spanning cluster. The spanning cluster typically has 
an immensely complex geometry where there may at some places be a high degree of connectivity, and at other places there may be 
connectivity through only one narrow cannel. Theese places will, during simulations of flow on the spanning cluster be places where 
all the ``liquid'' will have to pass through. We call theese places the singly connected bonds.\\
Other subsets of the spanning cluster include dangling ends, which in principle are simply dead ends, and so-called blobs which is a 
place where we have two ``singly'' connected bonds connecting to each other.\\
We can measure the singly connected bonds by placing (random) walkers on the spanning cluster from two directions and let them walk 
across the spanning cluster. The singly connected bonds will now have been visited by both walkers. We typically measure the 
singly connected bonds by their mass (area), and we want to find out how they scale with system size. This is shown in figure 
\ref{singly_connected}. We see that this as well behaves as a power law with the exponent $M_{SC} \propto L^{D_{SC}}$, where we measure 
$D_{SC} \simeq 0.7745$.

\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{oppg_m_mass.png}
\caption{Mass scaling of the singly connected bonds for increasing system size. Estimate of MSC as a function of L for L = 2.(3:9) . 500 samples used per L.
}
\label{singly_connected}
\end{figure}



\section{Random walks}
Behaviour of $\langle r^2\rangle$ of a random walker on the percolating cluster. Compare with diffusion in a bulk fluid and in 
a nanoporous system.\\

Generally we can say that as long as the cluster we land upon is a spanning cluster, $\langle r^2\rangle$ will (after a brief 
overlinar period) scale linearly with the number of steps just like 


\end{document}

